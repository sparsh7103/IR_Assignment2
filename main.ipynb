{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import math\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Function to preprocess text (tokenization, lowercasing)\n","def preprocess(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Tokenize by splitting on non-alphanumeric characters\n","    tokens = [word for word in text.split() if word.isalpha()]\n","    return tokens"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Step 1: Building the index (dictionary and postings list)\n","def build_index(corpus_dir):\n","    index = defaultdict(list)\n","    doc_lengths = {}\n","    doc_ids = {}\n","    doc_id_counter = 0\n","    num_docs = 0\n","    doc_term_counts = defaultdict(int)\n","\n","    for doc in os.listdir(corpus_dir):\n","        if doc.endswith(\".txt\"):\n","            doc_id_counter += 1\n","            doc_path = os.path.join(corpus_dir, doc)\n","            with open(doc_path, 'r', encoding='utf-8') as f:\n","                content = f.read()\n","            tokens = preprocess(content)\n","            doc_ids[doc_id_counter] = doc\n","\n","            term_freq = defaultdict(int)\n","            for token in tokens:\n","                term_freq[token] += 1\n","\n","            # Store term frequencies in the index\n","            for term, freq in term_freq.items():\n","                index[term].append((doc_id_counter, freq))\n","                doc_term_counts[term] += 1\n","\n","            # Calculate document length for normalization (lnc)\n","            length = math.sqrt(sum((1 + math.log10(freq)) ** 2 for freq in term_freq.values()))\n","            doc_lengths[doc_id_counter] = length\n","            num_docs += 1\n","\n","    return index, doc_ids, doc_lengths, num_docs, doc_term_counts"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Step 2: Calculate query vector using ltc scheme\n","def calculate_query_vector(query, index, num_docs, doc_term_counts):\n","    query_tokens = preprocess(query)\n","    query_freq = defaultdict(int)\n","    query_vector = {}\n","    \n","    for token in query_tokens:\n","        query_freq[token] += 1\n","    \n","    for token, freq in query_freq.items():\n","        if token in index:\n","            df = doc_term_counts[token]  # document frequency\n","            idf = math.log10(num_docs / df) if df > 0 else 0\n","            query_vector[token] = (1 + math.log10(freq)) * idf\n","    \n","    return query_vector"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Step 3: Rank documents using cosine similarity (lnc.ltc)\n","def rank_documents(query_vector, index, doc_lengths, doc_ids):\n","    scores = defaultdict(float)\n","    \n","    # Compute document scores based on the query\n","    for term, q_weight in query_vector.items():\n","        if term in index:\n","            for doc_id, term_freq in index[term]:\n","                doc_weight = 1 + math.log10(term_freq)  # lnc: log normalization\n","                scores[doc_id] += q_weight * doc_weight\n","\n","    # Normalize by document length (cosine similarity)\n","    for doc_id in scores:\n","        scores[doc_id] /= doc_lengths[doc_id]\n","\n","    # Sort documents by score in descending order\n","    ranked_docs = sorted(scores.items(), key=lambda item: (-item[1], doc_ids[item[0]]))\n","\n","    # Return the top 10 documents\n","    return [(doc_ids[doc_id], score) for doc_id, score in ranked_docs[:10]]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Main function to run the retrieval system\n","def search(query, corpus_dir):\n","    index, doc_ids, doc_lengths, num_docs, doc_term_counts = build_index(corpus_dir)\n","    query_vector = calculate_query_vector(query, index, num_docs, doc_term_counts)\n","    ranked_docs = rank_documents(query_vector, index, doc_lengths, doc_ids)\n","    \n","    for doc, score in ranked_docs:\n","        print(f'{doc}: {score}')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["zomato.txt: 0.4551637619443647\n","messenger.txt: 0.19847259520587918\n","swiggy.txt: 0.17220065221004402\n","instagram.txt: 0.16618344256317588\n","reddit.txt: 0.1631677399464748\n","skype.txt: 0.14242074610830285\n","yahoo.txt: 0.12514341101779197\n","HP.txt: 0.11913717629567064\n","google.txt: 0.1133746983912937\n","youtube.txt: 0.10398359283972905\n"]}],"source":["# Path to the folder containing the documents\n","corpus_dir = './Corpus'  # Adjust the path if necessary\n","\n","# Input the query from the user\n","query = input(\"Enter your query: \")\n","\n","# Perform the search\n","search(query, corpus_dir)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
