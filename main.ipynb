{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import math\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to preprocess text (tokenization, lowercasing)\n","def preprocess(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Tokenize by splitting on non-alphanumeric characters\n","    tokens = [word for word in text.split() if word.isalpha()]\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 1: Building the index (dictionary and postings list)\n","def build_index(corpus_dir):\n","    index = defaultdict(list)\n","    doc_lengths = {}\n","    doc_ids = {}\n","    doc_id_counter = 0\n","    num_docs = 0\n","    doc_term_counts = defaultdict(int)\n","\n","    for doc in os.listdir(corpus_dir):\n","        if doc.endswith(\".txt\"):\n","            doc_id_counter += 1\n","            doc_path = os.path.join(corpus_dir, doc)\n","            with open(doc_path, 'r', encoding='utf-8') as f:\n","                content = f.read()\n","            tokens = preprocess(content)\n","            doc_ids[doc_id_counter] = doc\n","\n","            term_freq = defaultdict(int)\n","            for token in tokens:\n","                term_freq[token] += 1\n","\n","            # Store term frequencies in the index\n","            for term, freq in term_freq.items():\n","                index[term].append((doc_id_counter, freq))\n","                doc_term_counts[term] += 1\n","\n","            # Calculate document length for normalization (lnc)\n","            length = math.sqrt(sum((1 + math.log10(freq)) ** 2 for freq in term_freq.values()))\n","            doc_lengths[doc_id_counter] = length\n","            num_docs += 1\n","\n","    return index, doc_ids, doc_lengths, num_docs, doc_term_counts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 2: Calculate query vector using ltc scheme\n","def calculate_query_vector(query, index, num_docs, doc_term_counts):\n","    query_tokens = preprocess(query)\n","    query_freq = defaultdict(int)\n","    query_vector = {}\n","    \n","    for token in query_tokens:\n","        query_freq[token] += 1\n","    \n","    for token, freq in query_freq.items():\n","        if token in index:\n","            df = doc_term_counts[token]  # document frequency\n","            idf = math.log10(num_docs / df) if df > 0 else 0\n","            query_vector[token] = (1 + math.log10(freq)) * idf\n","    \n","    return query_vector"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 3: Rank documents using cosine similarity (lnc.ltc)\n","def rank_documents(query_vector, index, doc_lengths, doc_ids):\n","    scores = defaultdict(float)\n","    \n","    # Compute document scores based on the query\n","    for term, q_weight in query_vector.items():\n","        if term in index:\n","            for doc_id, term_freq in index[term]:\n","                doc_weight = 1 + math.log10(term_freq)  # lnc: log normalization\n","                scores[doc_id] += q_weight * doc_weight\n","\n","    # Normalize by document length (cosine similarity)\n","    for doc_id in scores:\n","        scores[doc_id] /= doc_lengths[doc_id]\n","\n","    # Sort documents by score in descending order\n","    ranked_docs = sorted(scores.items(), key=lambda item: (-item[1], doc_ids[item[0]]))\n","\n","    # Return the top 10 documents\n","    return [(doc_ids[doc_id], score) for doc_id, score in ranked_docs[:10]]"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
